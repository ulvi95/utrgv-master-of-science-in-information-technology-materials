<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <title>Convert a physical chessboard into a digital one</title> <link rel="shortcut icon" href="https://tech.bakkenbaeck.com/assets/favicon.png"> <meta name="description" content="We have quite a few chess enthusiasts at our Oslo office, so we thought it would be cool to make something interesting for ourselves: a chessboard scanner th..."> <meta property="og:site_name" content="Bakken &amp; Bæck Tech"> <meta property="og:title" content="Convert a physical chessboard into a digital one"> <meta property="og:description" content="We have quite a few chess enthusiasts at our Oslo office, so we thought it would be cool to make something interesting for ourselves: a chessboard scanner th..."> <meta property="og:image" content="https://tech.bakkenbaeck.com/assets/meta.png"> <meta property="og:image:width" content="2024"> <meta property="og:image:height" content="1012"> <meta property="og:image:type" content="image/png"> <link rel="stylesheet" href="Convert%20a%20physical%20chessboard%20into%20a%20digital%20one_files/main.css"> <link rel="canonical" href="https://tech.bakkenbaeck.com/post/chessvision"> <link rel="alternate" type="application/rss+xml" title="Bakken &amp; Bæck Tech" href="https://tech.bakkenbaeck.com/feed.xml"> <link rel="preload" href="https://tech.bakkenbaeck.com/assets/fonts/GT-America-Regular.woff" as="font" type="font/woff" crossorigin=""> <link rel="preload" href="https://tech.bakkenbaeck.com/assets/fonts/GT-America-Bold.woff" as="font" type="font/woff" crossorigin=""> <script charset="utf-8" src="Convert%20a%20physical%20chessboard%20into%20a%20digital%20one_files/analytics.js"></script><style>._3emE9--dark-theme .-S-tR--ff-downloader{background:rgba(30,30,30,.93);border:1px solid rgba(82,82,82,.54);box-shadow:0 4px 7px rgba(30,30,30,.55);color:#fff}._3emE9--dark-theme .-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn{background:#3d4b52}._3emE9--dark-theme .-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn:hover{background:#131415}._3emE9--dark-theme .-S-tR--ff-downloader ._10vpG--footer{background:rgba(30,30,30,.93)}._2mDEx--white-theme .-S-tR--ff-downloader{background:#fff;border:1px solid rgba(82,82,82,.54);box-shadow:0 4px 7px rgba(30,30,30,.55);color:#314c75}._2mDEx--white-theme .-S-tR--ff-downloader ._6_Mtt--header{font-weight:700}._2mDEx--white-theme .-S-tR--ff-downloader ._2dFLA--container ._2bWNS--notice{border:0;color:rgba(0,0,0,.88)}._2mDEx--white-theme .-S-tR--ff-downloader ._10vpG--footer{background:#fff}.-S-tR--ff-downloader{display:block;overflow:hidden;position:fixed;bottom:20px;right:7.1%;width:330px;height:180px;background:rgba(30,30,30,.93);border-radius:2px;color:#fff;z-index:99999999;border:1px solid rgba(82,82,82,.54);box-shadow:0 4px 7px rgba(30,30,30,.55);transition:.5s}.-S-tR--ff-downloader._3M7UQ--minimize{height:62px}.-S-tR--ff-downloader._3M7UQ--minimize .nxuu4--file-info,.-S-tR--ff-downloader._3M7UQ--minimize ._6_Mtt--header{display:none}.-S-tR--ff-downloader ._6_Mtt--header{padding:10px;font-size:17px;font-family:sans-serif}.-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn{float:right;background:#f1ecec;height:20px;width:20px;text-align:center;padding:2px;margin-top:-10px;cursor:pointer}.-S-tR--ff-downloader ._6_Mtt--header ._2VdJW--minimize-btn:hover{background:#e2dede}.-S-tR--ff-downloader ._13XQ2--error{color:red;padding:10px;font-size:12px;line-height:19px}.-S-tR--ff-downloader ._2dFLA--container{position:relative;height:100%}.-S-tR--ff-downloader ._2dFLA--container .nxuu4--file-info{padding:6px 15px 0;font-family:sans-serif}.-S-tR--ff-downloader ._2dFLA--container .nxuu4--file-info div{margin-bottom:5px;width:100%;overflow:hidden}.-S-tR--ff-downloader ._2dFLA--container ._2bWNS--notice{margin-top:21px;font-size:11px}.-S-tR--ff-downloader ._10vpG--footer{width:100%;bottom:0;position:absolute;font-weight:700}.-S-tR--ff-downloader ._10vpG--footer ._2V73d--loader{animation:n0BD1--rotation 3.5s linear forwards;position:absolute;top:-120px;left:calc(50% - 35px);border-radius:50%;border:5px solid #fff;border-top-color:#a29bfe;height:70px;width:70px;display:flex;justify-content:center;align-items:center}.-S-tR--ff-downloader ._10vpG--footer ._24wjw--loading-bar{width:100%;height:18px;background:#dfe6e9;border-radius:5px}.-S-tR--ff-downloader ._10vpG--footer ._24wjw--loading-bar ._1FVu9--progress-bar{height:100%;background:#8bc34a;border-radius:5px}.-S-tR--ff-downloader ._10vpG--footer ._2KztS--status{margin-top:10px}.-S-tR--ff-downloader ._10vpG--footer ._2KztS--status ._1XilH--state{float:left;font-size:.9em;letter-spacing:1pt;text-transform:uppercase;width:100px;height:20px;position:relative}.-S-tR--ff-downloader ._10vpG--footer ._2KztS--status ._1jiaj--percentage{float:right}</style></head> <body class="white bg-near-black font-loaded"> <header class="header w-100 pb3 absolute pt3 pt5-l headroom headroom--not-bottom headroom--pinned headroom--top"> <div class="mw9 w-90 mw8-l center flex justify-between items-center"> <h1> <a class="logo" href="https://tech.bakkenbaeck.com/" data-internal="true"> <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1387.3 309.75"><g data-name="Layer 2"><g data-name="Layer 2"><path fill="#fff" d="M332.87 154.87c0 96.64-71.44 154.88-166.43 154.88S0 251.51 0 154.87 71.45 0 166.44 0s166.43 58.23 166.43 154.87zm-21.89 0c0-84.25-62.36-135.46-144.54-135.46S21.89 70.62 21.89 154.87s62.36 135.47 144.55 135.47S311 239.12 311 154.87zM698.79 154.87c0 96.64-71.45 154.88-166.44 154.88s-166.44-58.24-166.44-154.88S437.36 0 532.35 0s166.44 58.23 166.44 154.87zm-21.89 0c0-84.25-62.36-135.46-144.55-135.46S387.8 70.62 387.8 154.87s62.36 135.47 144.55 135.47S676.9 239.12 676.9 154.87z"></path><path fill="#fff" d="M251.86 194.33c0 31.26-23.12 49.81-61.75 49.81h-92V63.71h88.69c36.84 0 60 18.3 60 47.52 0 19.57-11.95 34.82-31.26 39.65 23.36 4.06 36.32 19.82 36.32 43.45zM123 84.55v57.18h62.26c24.14 0 36.59-9.91 36.59-29 0-18-13.21-28.21-36.59-28.21zm104 108c0-19.56-13-30-37.1-30H123v60.75h65.31c25.18 0 38.69-10.67 38.69-30.75zM617.86 194.33c0 31.26-23.12 49.81-61.75 49.81h-92V63.71h88.69c36.84 0 60 18.3 60 47.52 0 19.57-11.95 34.82-31.26 39.65 23.36 4.06 36.32 19.82 36.32 43.45zM489 84.55v57.18h62.26c24.14 0 36.59-9.91 36.59-29 0-18-13.21-28.21-36.59-28.21zm104 108c0-19.56-13-30-37.1-30H489v60.75h65.31c25.18 0 38.69-10.67 38.69-30.75zM920.2 84.08h-68.83v159.51h-24.89V84.08h-68.84V63.25H920.2zM1053.55 186.69H936.2c.76 25.4 19.31 41.41 50 41.41 22.61 0 38.61-8.64 47-25.15l18.79 8.64c-12.19 23.62-34.54 35.81-66.8 35.81-44.45 0-71.88-23.88-71.88-65.79 0-43.94 27.94-68.83 70.87-68.83 43.18 0 69.34 21.84 69.34 61.21zm-23.11-19.3c0-24.38-19.56-37.59-46.49-37.59-28.44 0-46.48 15.24-47.75 39.37h94.24zM1079.71 180.09c0-41.4 27.94-67.31 72.13-67.31 37.09 0 60 16.26 66 47l-23.62 2.79c-4.32-20.06-19-30.48-43.18-30.48-30 0-47.5 17.53-47.5 47.76 0 30.48 18 48.26 48 48.26 23.11 0 37.85-10.16 43.94-30.23l22.1 3.81c-6.35 29-31 45.72-66.55 45.72-43.13-.01-71.32-26.41-71.32-67.32zM1254.71 59.44h22.36v84.07c10.16-20.06 29-30.73 54.1-30.73 36.83 0 56.13 21.34 56.13 62.23v68.58H1365V177.3c0-29.72-14-45.22-40.89-45.22-28.71 0-47 18.8-47 49v62.48h-22.36z"></path></g></g></svg> </a> </h1> <nav class="flex items-center"> <a href="https://tech.bakkenbaeck.com/people/" class="people-link pa1 pr2 pr5-ns no-underline bn ff--inherit f5 fw3 bg-transparent white">People</a> <button class="pa1 p0 bn b0 ff--inherit f5 fw3 bg-transparent white" id="toggle-categories">Categories</button> </nav> </div> </header> <nav class="nav-categories w-100 h-100 fixed left-0 top-0"> <div class="nav-wrap w-100 h-100 absolute top-0 left-0"> <div class="mw9 w-90 mw8-l center cf"> <ul class="nav-list w-100 w-75-l fr-l f1 fw2 ttc"> <li class="dib br2"> <a class="white no-underline" href="https://tech.bakkenbaeck.com/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">[]L</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/java/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">W]{s</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/kotlin/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">^pE~M^</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/html/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">%qI}</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/front-end/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">Ni:EU%P]#</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/python/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">#]Lv}C</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/tornado/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">C;ZC&amp;%G</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/pytest/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">|n^bqH</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/android/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">f{q:~^u</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/architecture/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">[~EFs;_QEc_g</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/deep-learning/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">&amp;L%Yu||drS!]E</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/git/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">HOo</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/react/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">@:]hV</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/swift/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">#:$!!</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/c/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">l</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/sqlite/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">;Pnade</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/computer-vision/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">^{zq;{{RG!#|}Fg</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/chess/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">Ha;:y</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/animation/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">|x{DEgwMD</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/android/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">S?VWwCY</a> </li> <li> <a class="white no-underline" href="https://tech.bakkenbaeck.com/category/flow/" data-internal="true" data-scramble-text-idling="" data-scramble-text-running="">qUMM</a> </li> </ul> </div> </div> </nav> <main> <div class="content content--post w-90 mw8-l center" id="main"> <div class="mw9 center"> <article class="article f5 lh-copy cf" itemscope="" itemtype="http://schema.org/BlogPosting"> <div class="fl w-100 w-70-l moon-gray article-content"> <h2 class="f2 f1-ns lh-solid fw2 pb4 white"> <a class="white no-underline" href="https://tech.bakkenbaeck.com/post/chessvision" data-internal="true"> Convert a physical chessboard into a digital one </a> </h2> <p>We
 have quite a few chess enthusiasts at our Oslo office, so we thought it
 would be cool to make something interesting for ourselves: a chessboard
 scanner that converts the image of a physical chessboard at any given 
(chess) position into a digital chessboard! Then we could record the 
current state of the game and continue playing on our own devices, or 
share it with friends.</p> <p>In this article I’ll go through the 
journey of building the chessboard scanner. Let’s start with 
understanding the problem we are trying to solve - <em>we want to get a digital copy of a physical chessboard from an image</em>.</p> <p><img src="Convert%20a%20physical%20chessboard%20into%20a%20digital%20one_files/input_and_expected_result.jpg" alt="input-final-output"> <em>Expected input-output flow</em></p> <p>We can break this problem down into three sections:</p> <ul> <li>Chessboard recognition from a given image</li> <li>Identifying the chessboard position, orientation, square color, etc</li> <li>Chess piece recognition</li> </ul> <h3 id="chessboard-recognition">Chessboard recognition</h3> <p>Chessboard
 (and chess piece) recognition from a given image is an obvious 
candidate for computer vision. Though precise positioning of the 
chessboard using computer vision is quite challenging, there have been 
few attempts to solve this problem<sup id="fnref:mac18"><a href="#fn:mac18" class="footnote">1</a></sup> <sup id="fnref:zol18"><a href="#fn:zol18" class="footnote">2</a></sup>. The algorithm proposed by Maciej A. Czyzewskia et al. for chessboard recognition<sup id="fnref:mac18:1"><a href="#fn:mac18" class="footnote">1</a></sup>
 stands out. Their solution is based on generating a heat map to 
calculate the probability of a chessboard being located in a subsection 
of the image and cropping a tetragonal sub-area with highest probability
 values. This step is iterated to get the final square image of the 
chessboard. Remarkably, this approach is not affected by poor lighting 
conditions, the type of the chessboard, the image capturing angle nor 
damage to the chessboard. According to a comparison in their paper, this
 solution outperforms others. They have also benevolently published 
their <a href="https://github.com/maciejczyzewski/neural-chessboard">code</a>. With a few tweaks of our own we tested the algorithm and found it to be working really well.</p> <p><img src="Convert%20a%20physical%20chessboard%20into%20a%20digital%20one_files/input_and_2d_chessboard.jpg" alt="input"> <em>Chessboard recognition</em></p> <p>Employing
 the heat map approach, we can now easily crop a 2d projection of a 
chessboard into 64 individual squares. The next step is thus detecting 
whether a square is dark or light.</p> <h3 id="square-color-recognition">Square color recognition</h3> <p>Luckily
 for us, the color pattern of a typical chessboard is very 
straightforward. If we can determine just a single square with 
reasonable accuracy, then we can assign colors to the rest of the 
squares. With only two possible colors on a chessboard, binarizing the 
square should tell us if a square is light or dark.</p> <p>Since we 
can’t have complete control over the brightness and other conditions, 
the extracted square image can be noisy, and simple image thresholding 
therefore won’t always provide clear results. To remove the noise <a href="https://docs.opencv.org/master/d7/d4d/tutorial_py_thresholding.html">Otsu’s Binarization</a> can be used as follows:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">cv2</span> <span class="kn">as</span> <span class="nn">cv</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s">"noisy_square_img.png"</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">blur</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">GaussianBlur</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">img_binary</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">blur</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_BINARY</span><span class="o">+</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_OTSU</span><span class="p">)</span>
</code></pre></div> <p>This way we can calculate the exact amount of 
dark and light portions on a square. Given that there can be maximum of 
32 pieces on a chessboard, we will always have at least 32 empty squares
 which can be used to determine the reference square color. We will also
 set the acceptance threshold to 95 % to make sure that even in poor 
lighting conditions, all the square colors are detectable with high 
accuracy.</p> <p><img src="Convert%20a%20physical%20chessboard%20into%20a%20digital%20one_files/board_to_binary.jpg" alt="input"> <em>Output from Otsu’s binarization</em></p> <h3 id="chess-piece-recognition">Chess piece recognition</h3> <p>Here
 comes the fun part! We’ll use the magic of computer vision to recognize
 chess pieces on a square. Even though computer vision is around 60 
years old, the last decade has seen tons of new research and development
 within the field. Often you’ll find use of <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">Convolutional Neural Network</a> (CNN) in computer vision algorithms. We’ll be re-training an existing CNN for piece recognition.</p> <p>Training
 a CNN usually requires a sizeable amount of data, and unfortunately 
there is no dataset available that we can use 😔. This means we’ll have 
to build our own dataset!</p> <h4 id="1-building-a-dataset">1. Building a dataset</h4> <p>To
 make sure that our CNN works well on all sorts of chessboards we need 
some diversity in the dataset. This means we’ll need images with 
different angles, brightness, positions, etc. on different types of 
chessboards. For each image we get a 2D projection of a chessboard in 
the image using the chessboard recognition algorithm explained <a href="#chessboard-recognition">above</a>. Next we crop the 2D projection into 64 individual squares, thus extracting 64 data points from a single image.</p> <p>While
 taking pictures of different chessboards we realized that annotating 
all these images manually will be laborious and time consuming. Luckily 
we thought of an elegant solution to label all the images without any 
manual intervention: In the classical format of chess, players are asked
 to record the game with <a href="https://en.wikipedia.org/wiki/Chess_notation">algebraic notation</a>.
 We can re-play these recorded games and take a picture after each move.
 While splitting the 2D projected chessboard into 64 images, each image 
needs to be saved with an index from 0 to 63. Each index corresponds to 
the position of the square on the chessboard. Using <a href="https://github.com/niklasf/python-chess">python-chess</a> we walk through the recorded game and label images as follows:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span><span class="o">,</span> <span class="nn">glob</span>
<span class="kn">import</span> <span class="nn">chess.pgn</span>

<span class="c"># read the game file and set board</span>
<span class="n">pgn</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">"game.pgn"</span><span class="p">)</span>
<span class="n">game</span> <span class="o">=</span> <span class="n">chess</span><span class="o">.</span><span class="n">pgn</span><span class="o">.</span><span class="n">read_game</span><span class="p">(</span><span class="n">pgn</span><span class="p">)</span>
<span class="n">board</span> <span class="o">=</span> <span class="n">game</span><span class="o">.</span><span class="n">board</span><span class="p">()</span>

<span class="c"># read chessboard image at move x, jump board to move x</span>
<span class="n">filename</span> <span class="o">=</span> <span class="s">"move_x.jpg"</span>
<span class="n">move</span> <span class="o">=</span> <span class="n">game</span><span class="o">.</span><span class="n">mainline_moves</span><span class="p">()</span>
<span class="n">board</span><span class="o">.</span><span class="n">push</span><span class="p">(</span><span class="n">move</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">64</span><span class="p">):</span>
    <span class="n">piece</span> <span class="o">=</span> <span class="n">board</span><span class="o">.</span><span class="n">piece_at</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">piece</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c"># save image with empty label</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c"># save image with piece.symbol() label</span>
</code></pre></div> <p>With this simple script we managed to label 
123,008 images in a matter of minutes. I personally feel this part was 
the highlight of the project. 🎊</p> <h4 id="2-training-the-cnn">2. Training the CNN</h4> <p>Now
 it’s time to train our image classifier. There are quite a few options 
these days to do out-of-the-box machine learning. Personally I like <a href="https://keras.io/">Keras</a> to quickly build and test something.</p> <p>At
 any moment during a game, at least 50 % of the chessboard is empty 
squares which means that a minimum of 50% of the images in our dataset 
are squares without a piece. Since training requires a lot of data, we 
need to artificially extend the size of the training dataset. This can 
be easily done using the <a href="https://keras.io/preprocessing/image/">ImageDataGenerator</a> class as follows:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.image</span> <span class="kn">import</span> <span class="n">ImageDataGenerator</span>
<span class="kn">from</span> <span class="nn">keras.applications.inception_resnet_v2</span> <span class="kn">import</span> <span class="n">preprocess_input</span>

<span class="n">original_dataset_dir</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">labeled_dataset</span><span class="o">&gt;</span>

<span class="n">base_dir</span> <span class="o">=</span> <span class="o">&lt;</span><span class="n">base_dir</span><span class="o">&gt;</span>
<span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">base_dir</span><span class="p">)</span>

<span class="c"># Directories for our training, validation and test splits</span>
<span class="n">train_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s">'train'</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">train_dir</span><span class="p">)</span>

<span class="n">validation_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s">'validation'</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">validation_dir</span><span class="p">)</span>

<span class="n">test_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">base_dir</span><span class="p">,</span> <span class="s">'test'</span><span class="p">)</span>
<span class="n">os</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">test_dir</span><span class="p">)</span>

<span class="n">train_datagen</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span>
      <span class="n">preprocess_input</span><span class="o">=</span><span class="n">preprocess_input</span><span class="p">,</span>
      <span class="n">rotation_range</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
      <span class="n">shear_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
      <span class="n">zoom_range</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
      <span class="n">vertical_flip</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
      <span class="n">horizontal_flip</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="c"># the validation data should not be augmented!</span>
<span class="n">test_datagen</span> <span class="o">=</span> <span class="n">ImageDataGenerator</span><span class="p">(</span><span class="n">preprocess_input</span><span class="o">=</span><span class="n">preprocess_input</span><span class="p">)</span>
</code></pre></div> <p>Given the relatively small dataset we decided to 
use transfer learning, it’s better to re-train a few layers of a 
pre-trained model instead of training any model from scratch. Keras 
offers quite a few pre-trained models to choose from. We chose <a href="https://github.com/tensorflow/models/blob/master/research/slim/nets/inception_resnet_v2.py">Inception-ResNet-v2</a>
 as our base model, freezing the first 249 layers and re-training the 
remaining ones with our dataset. We did experiment with VGG and others<sup id="fnref:kar15"><a href="#fn:kar15" class="footnote">3</a></sup> as our base model but <code class="highlighter-rouge">Inception-ResNet-v2</code> performed significantly better than the rest.</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">keras</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">layers</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">models</span>
<span class="kn">from</span> <span class="nn">keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">keras.applications.inception_resnet_v2</span> <span class="kn">import</span> <span class="n">InceptionResNetV2</span>

<span class="n">base_model</span> <span class="o">=</span> <span class="n">InceptionResNetV2</span><span class="p">(</span><span class="n">include_top</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="s">'imagenet'</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">150</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">base_model</span><span class="o">.</span><span class="n">output</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">GlobalAveragePooling2D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span> <span class="c"># New softmax layer</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">base_model</span><span class="o">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">predictions</span><span class="p">)</span>

<span class="c"># we chose to train the top 2 inception blocks</span>
<span class="c"># we will freeze the first 249 layers and unfreeze the rest</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[:</span><span class="mi">249</span><span class="p">]:</span>
   <span class="n">layer</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>
<span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">249</span><span class="p">:]:</span>
   <span class="n">layer</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">True</span>

<span class="n">adam</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">0.0001</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">adam</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</code></pre></div> <p>Now we just need to train our model using <code class="highlighter-rouge">fit_generator</code> on augmented training data <code class="highlighter-rouge">train_generator</code>. We have divided the chess pieces into six different groups: <code class="highlighter-rouge">bishop</code>, <code class="highlighter-rouge">empty</code>, <code class="highlighter-rouge">king_or_queen</code>, <code class="highlighter-rouge">knight</code>, <code class="highlighter-rouge">pawn</code>, <code class="highlighter-rouge">rook</code>. The group <code class="highlighter-rouge">king_or_queen</code> consists of <code class="highlighter-rouge">queen</code> and <code class="highlighter-rouge">king</code> since it often can be difficult even for human eyes to distinguish <code class="highlighter-rouge">queen</code> from <code class="highlighter-rouge">king</code> in an image</p> <p><img src="Convert%20a%20physical%20chessboard%20into%20a%20digital%20one_files/model_result.png" alt="input"> <em>Overview of the training run</em></p> <h3 id="determining-chess-piece-color">Determining chess piece color</h3> <p>For
 detecting the color of a single piece we can again make use of image 
processing, similar to what we did for square color. However, simple 
binarization of the image won’t really work here, since there can be lot
 of noise on an image of a square with a piece on it. To tackle this we 
make use of <a href="https://docs.opencv.org/trunk/d9/d61/tutorial_py_morphological_ops.html">Morphological transformation</a> as follows:</p> <div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">cv2</span> <span class="kn">as</span> <span class="nn">cv</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s">"noisy_piece_img.png"</span><span class="p">,</span><span class="mi">0</span><span class="p">)</span>
<span class="n">blur</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">GaussianBlur</span><span class="p">(</span><span class="n">img</span><span class="p">,(</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span><span class="mi">0</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">img_binary</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">threshold</span><span class="p">(</span><span class="n">blur</span><span class="p">,</span><span class="mi">130</span><span class="p">,</span><span class="mi">255</span><span class="p">,</span><span class="n">cv2</span><span class="o">.</span><span class="n">THRESH_BINARY</span><span class="p">)</span>

<span class="n">img_binary_inverted</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">bitwise_not</span><span class="p">(</span><span class="n">img_binary</span><span class="p">)</span>

<span class="c"># remove noise</span>
<span class="n">morph_kernel</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">15</span><span class="p">,</span><span class="mi">15</span><span class="p">),</span><span class="n">np</span><span class="o">.</span><span class="n">unit8</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">morphologyEX</span><span class="p">(</span><span class="n">img_binary_inverted</span><span class="p">,</span> <span class="n">cv2</span><span class="o">.</span><span class="n">MORPH_CLOSE</span><span class="p">,</span> <span class="n">morph_kernel</span><span class="p">)</span>
</code></pre></div> <p><img src="Convert%20a%20physical%20chessboard%20into%20a%20digital%20one_files/morphology_board.jpg" alt="input"> <em>Morphological transformation</em></p> <p>As
 we already know the color of the background square and whether it is 
occupied or empty, we can easily find the color of the piece.</p> <h3 id="creating-a-digital-board">Creating a digital board</h3> <p>Now that we have all the information it’s time to glue everything together to create our digital chessboard copy. <a href="#training-the-cnn">Earlier</a> we combined <code class="highlighter-rouge">king</code> and <code class="highlighter-rouge">queen</code>
 in a single group which means there can be maximum of four possible 
outputs. We find the most likely output by combining chess rules, a 
chess engine and the probabilities we got from the model. Here’s a list 
of rules we used:</p> <ul> <li>There can not be more than 32 pieces on a chessboard</li> <li>There can be a maximum of 16 pieces for a color</li> <li>At all times we need one king of each color on the board</li> <li>For each color, the total number of pawns and queen can not exceed nine</li> <li>For each color, the total number of pawns and piece except queen or king can not exceed ten</li> <li>You can not have pawns in the back rank (first and last row on a chessboard)</li> </ul> <p>For mostly likely output, we generate the <a href="https://en.wikipedia.org/wiki/Forsyth%E2%80%93Edwards_Notation">FEN</a> which will be later used to create the digital board, all we are missing now is UI to visualize the output.</p> <p>However
 when we looked into existing 3rd party/open source chessboard editors 
for Android, we found that they were either outdated, too complicated or
 both. Our talented mobile team used this opportunity to build a new 
component that can read and parse the FEN representation of a board and 
allows users to freely place/move pieces on a chess board using drag and
 drop. It’s also equipped to return the edited board in FEN 
representation. Our <a href="https://github.com/bakkenbaeck/chessboardeditor">new lightweight library</a> is Kotlin based and is meant to work on the latest Android version (with backwards compatibility to <code class="highlighter-rouge">Android 5.0</code>).</p> <p>We have also been working on an Android app for the project and here’s a glimpse of the MVP:</p> <p><img src="Convert%20a%20physical%20chessboard%20into%20a%20digital%20one_files/chessvision_app.gif" alt="input" style="display: block;margin-left:auto;margin-right: auto; width: 300px; height: 579px"></p> <h3 id="conclusion">Conclusion</h3> <p>It’s
 worth mentioning that we deployed everything to a GPU workstation 
equipped with Nvidia GeForce GTX 1080 Ti. With this setup, it takes 
about 10 seconds to process an image in order to get a digital copy of 
the chessboard. Looking back, lot of challenges we tackled during this 
project involved quite a bit of research and experimentation, like 
creating our own dataset, building a chessboard editor and successfully 
creating a digital copy of a chessboard from an image, with fairly good 
accuracy. With more data and research/experiments on model architecture 
we should be able improve performance of the piece recognition even 
further.</p> <p>This is just the beginning of ChessVision and we are 
really excited about expanding its scope to wider use cases such as 
scanning a chessboard from a book, converting the video recording of a 
chess game to a digital copy (which can be exported to chess engines) 
and more.</p> <h4 id="references">References</h4> <div class="footnotes"> <ol> <li id="fn:mac18"> <p>Maciej A. Czyzewskia, Artur Laskowski, Szymon Wasik: <a href="https://arxiv.org/pdf/1708.03898.pdf">Chessboard and chess piece recognition with the support of neural networks</a>. 2018.&nbsp;<a href="#fnref:mac18" class="reversefootnote">↩</a>&nbsp;<a href="#fnref:mac18:1" class="reversefootnote">↩<sup>2</sup></a></p> </li> <li id="fn:zol18"> <p>Zoltán Orémuš: <a href="https://is.muni.cz/th/meean/Master_Thesis.pdf">Chess Position Recognition from a Photo</a>. 2018.&nbsp;<a href="#fnref:zol18" class="reversefootnote">↩</a></p> </li> <li id="fn:kar15"> <p>Karen Simonyan, Andrew Zisserman: <a href="https://arxiv.org/pdf/1409.1556.pdf">Very Deep Convolutional Networks for Large-Scale Image Recognition</a>. 2015.&nbsp;<a href="#fnref:kar15" class="reversefootnote">↩</a></p> </li> </ol> </div> </div> <div class="fl w-100 w-30-l pl6-ns"> <a href="https://tech.bakkenbaeck.com/people/saurabh-b" class="no-underline"> <img class="avatar-medium br-100 mb2" src="Convert%20a%20physical%20chessboard%20into%20a%20digital%20one_files/saurabh-b.jpg" alt="Saurabh B"> <p class="fw4">Saurabh B</p> <p class="fw4 o-50 mb4">Developer</p> </a> <hr class="w1 ma0 mb4 bg-white o-40"> <ul class="tags list pl0 lh-copy f5 mb4"> <li class="db mb2"> <a class="no-underline white ttc dim" href="https://tech.bakkenbaeck.com/category/deep-learning/" data-internal="true"> deep-learning </a> </li> <li class="db mb2"> <a class="no-underline white ttc dim" href="https://tech.bakkenbaeck.com/category/computer-vision/" data-internal="true"> computer-vision </a> </li> <li class="db mb2"> <a class="no-underline white ttc dim" href="https://tech.bakkenbaeck.com/category/chess/" data-internal="true"> chess </a> </li> </ul> <hr class="w1 ma0 mb4 bg-white o-40"> <time class="o-50">Nov 13, 2019</time> </div> </article> <footer class="w-100 cf pt4 pb6"> <div class="w-100 w-75-l fl flex justify-between"> <a class="fl-m db tc mb4 white fw6 no-underline" href="https://tech.bakkenbaeck.com/post/room-insert-update" data-internal="true"> <strong class="pr2">← Previous:</strong> <span class="underline">Saving cats with Insert or Update in Room</span> </a> <a class="fr-m db tc white fw6 no-underline" href="https://tech.bakkenbaeck.com/post/android-animations" data-internal="true"> <strong class="pr2">Next:</strong> <span class="underline">Android Animations - interacting with the user</span> → </a> </div> </footer> </div> </div>  </main> <footer class="w-100 pb3 pb5-ns f5 lh-copy"> <div class="w-90 mw9 mw8-l center cf pt4 bt b--white-10"> <div class="fl w-100 w-25-l"> <h2 class="pb4"> <a class="white no-underline" href="http://bakkenbaeck.com/">Bakken &amp; Bæck</a> </h2> </div> <nav class="fl w-100 w-75-l cf"> <ul class="w-50 w-30-l fr"> <li> <a class="white no-underline dim" href="https://bakkenbaeck.com/join">Careers</a> </li> <li> <a class="white no-underline dim" href="https://medium.com/bakken-b%C3%A6ck">Blog</a> </li> <li> <a class="white no-underline dim" href="https://bakkenbaeck.com/about">About</a> </li> <li> <a class="white no-underline dim" href="https://bakkenbaeck.com/">Contact</a> </li> </ul> <ul class="w-50 w-30-l fr"> <li> <a class="white no-underline dim" href="http://facebook.com/bakkenbaeck">Facebook</a> </li> <li> <a class="white no-underline dim" href="http://twitter.com/bakkenbaeck">Twitter</a> </li> <li> <a class="white no-underline dim" href="http://github.com/bakkenbaeck">Github</a> </li> <li> <a class="white no-underline dim" href="http://instagram.com/bakkenbaeck">Instagram</a> </li> </ul> </nav> </div> </footer>  <script src="Convert%20a%20physical%20chessboard%20into%20a%20digital%20one_files/script.js"></script> 
</body></html>